一，下载网页:

import urllib2
def download ( url ) :
	return urllib2. urlopen (url) . read ( )



import urllib2
def download ( url ):
	print ’ Downloading ：’，url
	try:
		html = urllib2.urlopen(url). read( )
	except urllib2. URLError as e:
		print ’Download error：’ ，e. reason
		html = None
	return html

现在， 当出现下载错误时， 该函数能够捕获到异常， 然后返回None 。



1,重试下载



2.设置用户代理
def download (url, user_agent＝ ’wswp’，num_retries=2):
    print’ Downloading:’，url
    headers =｛ ’User-agent’: user_agent }
    request = urllib2.Request( u r l , he ade r s =headers)
try:
    html = urllib2.urlopen(request).read( )
except urllib2.URLError as e:
    print ’Download error：’ ，e.reason
    html = None
    if num_retries > 0:
        if hasattr(e, ’code’ ）and 500 <= e.code < 600:
	    # retry 5XX HTTP errors
	    return download(url, user_agent, num_retries-1)
return html



1.4.2 网站地图爬虫

1.4.3 ID遍历爬虫

1.4.4 链接爬虫

高级功能

  支持代理






二，数据抓取

首先， 我们会介绍一个叫做Firebug Lite 的浏览器扩展， 用于检查网页内容， 如果你有一些网络开发背景的话， 可能己经对该扩展十分熟悉了。然后，
我们会介绍三种抽取网页数据的方法， 分别是正则表达式、Beauti削Soup和lxml 。



2.1 分析网页


2.2.1 正则表达式

2.2.2 Beautifu l Sou p

2.2.3 Lxml

















